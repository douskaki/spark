{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gentle Intro to Apache Spark\n",
    "## What is Apache Spark\n",
    "Apache Spark is a *unified computing engine* and a set of *libraries* for parallel data processing on computer clusters.\n",
    "\n",
    "![](images/eco.png)\n",
    "\n",
    "## Philosophy\n",
    "1. Unified\n",
    "2. Computing engine\n",
    "3. Libraries\n",
    "\n",
    "# 2 Structure\n",
    "## 2.1 Driver vs. Executors\n",
    "\n",
    "![](images/architech.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master('local[2]')\\\n",
    "        .appName('Cha1')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSession and Language API\n",
    "\n",
    "![](images/sparksession.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "- most common Structured API\n",
    "- *schema*: list that defines the columns and the types within those columns\n",
    "![](images/df.png)\n",
    "\n",
    "## Partitions\n",
    "Paritions are collection of rows of the data stored in different clusters\n",
    "- A DataFrame’s partitions represent how the data is physically distributed across the cluster of machines during execution. \n",
    "- If you have one partition, Spark will have a parallelism of only one, even if you have thousands of executors. If you have many partitions but only one executor, Spark will still have a parallelism of only one because there is only one computation resource..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[number: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRange = spark.range(1000).toDF('number')\n",
    "myRange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[number: bigint]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divisBy2 = myRange.where('number % 2 = 0 ')\n",
    "divisBy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Narrow Tranformations  \n",
    "![](images/narrow-trans.png)\n",
    "\n",
    "- Wide Transformations (Shuffles)  \n",
    "![](images/wide-trans.png)\n",
    "\n",
    "#### Lazy Evaluation\n",
    "In Spark, instead of modifying the data immediately when we express some operation, we build up a plan of transformations that we would like to apply to our source data. This provides immense benefits to the end user because Spark can optimize the entire data flow from end to end.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions\n",
    "Trigger the tranformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divisBy2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Three kinds of tranformations\n",
    "- actions to view data in the console;\n",
    "- actions to collect data to native objects in the respective language;\n",
    "- and actions to write to output data sources\n",
    "\n",
    "## Spark UI\n",
    "Monitor spark jobs\n",
    "\n",
    "Local Port: http://localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An End-to-End example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path='../Spark-The-Definitive-Guide-master'\n",
    "data_path=\"/data/flight-data/csv/2015-summary.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015 = spark\\\n",
    "    .read\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .option('header', 'true')\\\n",
    "    .csv(''.join([dir_path, data_path]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/read_csv.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similar to df.head()\n",
    "flightData2015.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/sort.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [count#23 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#23 ASC NULLS FIRST, 200)\n",
      "   +- *(1) FileScan csv [DEST_COUNTRY_NAME#21,ORIGIN_COUNTRY_NAME#22,count#23] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/binb/Desktop/spark/Spark-The-Definitive-Guide-master/data/flight-dat..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "# call explain on any DataFrame object \n",
    "# to see the DataFrame’s lineage\n",
    "# how Spark will execute this query\n",
    "flightData2015.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executre the plan\n",
    "Make tranformation to action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## default: shuffle output 200 partitions\n",
    "## now set it to 5 to simplify\n",
    "spark.conf.set('spark.sql.shuffle.partitions', '5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.sort('count').take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/shuffle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with different partitions\n",
    "We do not manipulate the physical data; instead, we configure physical execution characteristics\n",
    "through things like the shuffle partitions parameter that we set a few moments ago. This control the physical execution characteristics of Spark jobs.\n",
    "\n",
    "In experimenting with different values, you should see drastically different runtimes. Remember that you can\n",
    "monitor the job progress by navigating to the Spark UI on port 4040 to see the physical and logical execution characteristics of your jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# partitions = 5\n",
    "spark.conf.set('spark.sql.shuffle.partitions', '5')\n",
    "flightData2015.sort('count').take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# partitions = 500\n",
    "spark.conf.set('spark.sql.shuffle.partitions', '500')\n",
    "flightData2015.sort('count').take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames and SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** DataFrames are immutable **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer DataFrame into a table or view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.createOrReplaceTempView('flight_data_2015')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay = spark.sql('''\n",
    "SELECT DEST_COUNTRY_NAME, count(1)\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME''')\n",
    "\n",
    "dataFrameWay = flightData2015\\\n",
    "    .groupBy('DEST_COUNTRY_NAME')\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#21], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#21, 500)\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#21], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [DEST_COUNTRY_NAME#21] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/binb/Desktop/spark/Spark-The-Definitive-Guide-master/data/flight-dat..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "====================================================================================================\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#21], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#21, 500)\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#21], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [DEST_COUNTRY_NAME#21] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/binb/Desktop/spark/Spark-The-Definitive-Guide-master/data/flight-dat..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n"
     ]
    }
   ],
   "source": [
    "sqlWay.explain()\n",
    "print('====='*20)\n",
    "dataFrameWay.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Two plans are the same**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ex1: Find the Maximum 'count'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## sql\n",
    "spark.sql('''\n",
    "SELECT max(count) \n",
    "FROM flight_data_2015''').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## python\n",
    "from pyspark.sql.functions import max\n",
    "flightData2015.select(max('count')).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ex2: find top five destination countries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sql\n",
    "maxSql = spark.sql('''\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY destination_total DESC\n",
    "LIMIT 5''')\n",
    "maxSql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# python\n",
    "from pyspark.sql.functions import desc\n",
    "flightData2015\\\n",
    "    .groupBy('DEST_COUNTRY_NAME')\\\n",
    "    .sum('count')\\\n",
    "    .withColumnRenamed('sum(count)', 'destination_total')\\\n",
    "    .sort(desc('destination_total'))\\\n",
    "    .limit(5)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution plan\n",
    "The true execution plan (the one visible in explain) will differ from that shown in Figure 2-10 because of optimizations in the physical execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execution plan is *directed acyclic graph*(DAG).  \n",
    "Each resulting DataFrame is immutable.  \n",
    "Action to generate result (i.e. `df.shows()`)\n",
    "\n",
    "![](images/ex2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **1st step**: *read* in the data.   \n",
    "The DataFrame is not read in until an action is called on that DataFrame or one derived from the original df.\n",
    "- **2nd step**: *grouping*.   \n",
    "`groupBy` creates a RelationalGroupedDataset, a DataFrame that has a grouping specified but needs the user to specify an aggregation before it can be queried further. Grouping by a key (or set of keys) and then perform an aggregation over each one of those keys.\n",
    "- **3rd step**: specify the *aggregation*.   \n",
    "`sum` aggregation method: takes as input a column expression or a column name. The result of the sum\n",
    "method call is a *new DataFrame.* with new *schema* which know the type of each column. (*tranformation*: no computation has been performed)\n",
    "- **4th step**: *renaming*.   \n",
    "`withColumnRenamed` method: takes two arguments (original column name, new column name). (*transformation*)\n",
    "- **5th step**: *sorts* the data  \n",
    "*transformation*  \n",
    "Note: (`desc` function: does not return a string but a Column. In general, many DataFrame methods will accept strings (as column names) or Column types or expressions. (they are the same)\n",
    "- **6th step**: specify a *limit*.   \n",
    "return the first five values in our final DataFrame \n",
    "- **7th step**: *action!*  \n",
    "process of collecting the results of our DataFrame, and Spark will give us back a list or array in the language that we’re executing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[destination_total#253L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#21,destination_total#253L])\n",
      "+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#21], functions=[sum(cast(count#23 as bigint))])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#21, 500)\n",
      "      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#21], functions=[partial_sum(cast(count#23 as bigint))])\n",
      "         +- *(1) FileScan csv [DEST_COUNTRY_NAME#21,count#23] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/binb/Desktop/spark/Spark-The-Definitive-Guide-master/data/flight-dat..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "## explain plan\n",
    "flightData2015\\\n",
    "    .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "    .sum('count')\\\n",
    "    .withColumnRenamed('sum(count)','destination_total')\\\n",
    "    .sort(desc('destination_total'))\\\n",
    "    .limit(5)\\\n",
    "    .explain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
