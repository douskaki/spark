{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A tour for Spark's toolset\n",
    "![](images/toolset.png)\n",
    "\n",
    "**TOC**\n",
    "- Running production applications with spark-submit\n",
    "- Datasets: type-safe APIs for structured data\n",
    "- Structured Streaming\n",
    "- Machine learning and advanced analytics\n",
    "- Resilient Distributed Datasets (RDD): Spark’s low level APIs\n",
    "- SparkR\n",
    "- The third-party package ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Production Applications\n",
    "\n",
    "- `spark-submit`: send application code to a cluster and excute there. (via cluster manager)\n",
    "- application run until it exits (complete the task) or encounters an error\n",
    "\n",
    "**Spark's cluster manager**\n",
    "- Standlone\n",
    "- Mesos\n",
    "- YARN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## not run here\n",
    "## scala version\n",
    "## only via Command Line under spark root directory\n",
    "./bin/spark-submit \\\n",
    "    --class org.apache.spark.examples.SparkPi \\\n",
    "    --master local \\\n",
    "    ./examples/jars/spark-examples_2.11-2.2.0.jar 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample application calculates the digits of pi to a certain level of estimation. Here, we’ve told spark-submit that we want to run on our local machine, which class and which JAR we would like to run, and some command-line arguments for that class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## not run here\n",
    "## python version\n",
    "./bin/spark-submit \\\n",
    "    --master local \\\n",
    "    ./examples/src/main/python/pi.py 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing the master argument of `spark-submit`, we can also submit the same application to a cluster running Spark’s standalone cluster manager, Mesos or YARN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets: Type-Safe Structured APIs\n",
    "- Only for **statically typed** code: Java and Scale  \n",
    "- Not available for **dynamically typed** language: Python and R\n",
    "\n",
    "Similar to Java `ArrayList` or Scala `Seq`\n",
    "- APIs are *type-safe*\n",
    "\n",
    "*Come back in the future*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Streamming (Part V)\n",
    "- high-level API for stream processing.\n",
    "- reduce latency and allow incremental processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example - retail dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head ../Spark-The-Definitive-Guide-master/data/retail-data/by-day/2010-12-01.csv -n 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "    .master('local[3]')\\\n",
    "    .appName('Cha3')\\\n",
    "    .getOrCreate()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data as static DataFrame\n",
    "staticDataFrame = spark.read.format('csv')\\\n",
    "    .option('header', 'true')\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .load('../Spark-The-Definitive-Guide-master/data/retail-data/by-day/*.csv')\n",
    "\n",
    "staticDataFrame.createOrReplaceTempView(\"retail_data\")\n",
    "staticSchema = staticDataFrame.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerId|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|   14075.0|[2011-12-04 16:00...|316.78000000000003|\n",
      "|   18180.0|[2011-12-04 16:00...|            310.73|\n",
      "|   15358.0|[2011-12-04 16:00...| 830.0600000000003|\n",
      "|   15392.0|[2011-12-04 16:00...|304.40999999999997|\n",
      "|   15290.0|[2011-12-04 16:00...|263.02000000000004|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import window, column, desc, col\n",
    "# set partitions = 5 after shuffle\n",
    "spark.conf.set('spark.sql.shuffle.partitions', '5')\n",
    "\n",
    "# select query\n",
    "staticDataFrame\\\n",
    "    .selectExpr(\n",
    "    'CustomerId',\n",
    "    '(UnitPrice * Quantity) as total_cost',\n",
    "    'InvoiceDate')\\\n",
    "    .groupBy(\n",
    "    col('CustomerId'), window(col('InvoiceDate'), '1 day'))\\\n",
    "    .sum('total_cost')\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create streaming dataframe\n",
    "streamingDataFrame = spark.readStream\\\n",
    "    .schema(staticSchema)\\\n",
    "    .option('maxFilesPerTrigger', 1)\\\n",
    "    .format('csv')\\\n",
    "    .option('header', 'true')\\\n",
    "    .load('../Spark-The-Definitive-Guide-master/data/retail-data/by-day/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check streaming\n",
    "streamingDataFrame.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select query as above \n",
    "# lazy operation\n",
    "purchaseByCustomerPerHour = streamingDataFrame\\\n",
    "    .selectExpr(\n",
    "    'CustomerId',\n",
    "    '(UnitPrice * Quantity) as total_cost',\n",
    "    'InvoiceDate')\\\n",
    "    .groupBy(\n",
    "    col('CustomerId'), window(col('InvoiceDate'), '1 day'))\\\n",
    "    .sum('total_cost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streaming actions\n",
    "The action we will use will output to an in-memory table that\n",
    "we will update after each `trigger`. In this case, each trigger is based on an individual file (the read\n",
    "option that we set). \n",
    "\n",
    "Spark will mutate the data in the in-memory table such that we will always have the highest value as specified in our previous aggregation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f40e508cc18>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchaseByCustomerPerHour.writeStream\\\n",
    "    .format('memory')\\\n",
    "    .queryName('customer_purchases')\\\n",
    "    .outputMode('complete')\\\n",
    "    .start()\n",
    "    # memory = store in-memory table\n",
    "    # the name of the in-memory table\n",
    "    # complete = all the counts should be in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerId|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|      null|[2011-11-10 16:00...|13636.969999999936|\n",
      "|      null|[2011-01-23 16:00...|  8101.42000000001|\n",
      "|      null|[2011-01-30 16:00...|  4822.28000000001|\n",
      "|      null|[2011-11-11 16:00...| 4538.830000000009|\n",
      "|   15311.0|[2011-11-11 16:00...| 4041.180000000001|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# start the stream\n",
    "spark.sql('''\n",
    "SELECT *\n",
    "FROM customer_purchases\n",
    "ORDER BY `sum(total_cost)` DESC\n",
    "''')\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f40e508c320>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write the results to the console \n",
    "purchaseByCustomerPerHour.writeStream\\\n",
    "    .format(\"console\")\\\n",
    "    .queryName(\"customer_purchases_2\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning and Advanced Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "staticDataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning algorithms in MLlib require that data is represented as __numerical values__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation\n",
    "from pyspark.sql.functions import date_format, col\n",
    "preppedDataFrame = staticDataFrame\\\n",
    "    .na.fill(0)\\\n",
    "    .withColumn('day_of_week', date_format(col('InvoiceDate'), 'EEEE'))\\\n",
    "    .coalesce(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = false)\n",
      " |-- CustomerID: double (nullable = false)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preppedDataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split \n",
    "trainDataFrame = preppedDataFrame\\\n",
    "    .where('InvoiceDate < \"2011-07-01\"')\n",
    "testDataFrame = preppedDataFrame\\\n",
    "    .where('InvoiceDate >= \"2011-07-01\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainDF size : 245903\n",
      "TestDF size : 296006\n"
     ]
    }
   ],
   "source": [
    "print('TrainDF size : {}'.format(trainDataFrame.count()))\n",
    "print('TestDF size : {}'.format(testDataFrame.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark’s MLlib also provides a number of transformations with which we can automate some of our general transformations. One such transformer is a `StringIndexer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer()\\\n",
    "    .setInputCol('day_of_week')\\\n",
    "    .setOutputCol('day_of_week_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will turn our days of weeks into corresponding numerical values. For example, Spark might\n",
    "represent Saturday as 6, and Monday as 1. However, with this numbering scheme, we are\n",
    "implicitly stating that Saturday is greater than Monday (by pure numerical values). This is\n",
    "obviously *incorrect*. To fix this, we therefore need to use a `OneHotEncoder` to encode each of\n",
    "these values as their own column. These Boolean flags state whether that day of week is the\n",
    "relevant day of the week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "encoder = OneHotEncoder()\\\n",
    "    .setInputCol('day_of_week_index')\\\n",
    "    .setOutputCol('day_of_week_encoded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these will result in a set of columns that we will “assemble” into a vector. All machine\n",
    "learning algorithms in Spark take as **input** a `Vector` type, which must be a set of numerical\n",
    "values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = VectorAssembler()\\\n",
    "    .setInputCols(['UnitPrice', 'Quantity', 'day_of_week_encoded'])\\\n",
    "    .setOutputCol('features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have three key features: the price, the quantity, and the day of week. Next, we’ll set this\n",
    "up into a **pipeline** so that any future data we need to transform can go through the exact same\n",
    "process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up the pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "transformationPipeline = Pipeline()\\\n",
    "    .setStages([indexer, encoder, vectorAssembler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to fit our **transformers** to this dataset.\n",
    "(Cover in depth in *Part VI*) Basically our StringIndexer needs to know how many\n",
    "unique values there are to be indexed. After those exist, encoding is easy but Spark must look at all the distinct values in the column to be indexed in order to store those values later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedPipeline = transformationPipeline.fit(trainDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data\n",
    "transformedTraining = fittedPipeline.transform(trainDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ML** :There are always two types for every algorithm in MLlib’sDataFrame API. \n",
    "- `Algorithm` : untrained version\n",
    "- `AlgorithmModel` : trained version. \n",
    "    \n",
    "In our example, this is `KMeans` and then `KMeansModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison on w/ or w/o caching\n",
    "import timeit as t\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans()\\\n",
    "    .setK(20)\\\n",
    "    .setSeed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caching** : an optimization (more detail in *Part IV*). This will put a copy of the intermediately transformed dataset into memory, allowing us to repeatedly access it at much lower cost than running the entire pipeline again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time : -0.00886492900281155\n"
     ]
    }
   ],
   "source": [
    "# w/o caching\n",
    "t1 = t.timeit()\n",
    "\n",
    "kmModel = kmeans.fit(transformedTraining)\n",
    "\n",
    "t2 = t.timeit()\n",
    "print('Training time : {}'.format(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time : -0.012267105999853811\n"
     ]
    }
   ],
   "source": [
    "# w/ caching\n",
    "t1 = t.timeit()\n",
    "\n",
    "transformedTraining.cache()\n",
    "kmModel = kmeans.fit(transformedTraining)\n",
    "\n",
    "t2 = t.timeit()\n",
    "print('Training time : {}'.format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the cost according to some success merits on our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517507094.72221166"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformedTest = fittedPipeline.transform(testDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost of train set : 84553739.96537484 \n",
      "Cost of test set : 517507094.72221166\n"
     ]
    }
   ],
   "source": [
    "print('Cost of train set : {}'.format(kmModel.computeCost(transformedTraining)),\n",
    "      '\\nCost of test set : {}'.format(kmModel.computeCost(transformedTest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower Level APIs (Part IV)\n",
    "\n",
    "Spark includes a number of lower-level primitives to allow for arbitrary Java and Python object\n",
    "manipulation via **Resilient Distributed Datasets (RDDs)** (Chapter 4). Virtually everything in Spark is built on\n",
    "top of RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usage 1**: parallelize raw data that you have stored in memory on the driver machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: bigint]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "spark.sparkContext.parallelize([Row(1), Row(2), Row(3)]).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: There are basically no instances in modern Spark, for which you\n",
    "should be using RDDs instead of the structured APIs beyond manipulating some very raw\n",
    "unprocessed and unstructured data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkR (Part VII)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%R` not found.\n"
     ]
    }
   ],
   "source": [
    "%%R ## need to set up R kernel for jupyter notebook\n",
    "library(SparkR)\n",
    "\n",
    "sparkDF <- read.df(\"/data/flight-data/csv/2015-summary.csv\",\n",
    "                   source = \"csv\", header=\"true\", inferSchema = \"true\")\n",
    "take(sparkDF, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "collect(orderBy(sparkDF, \"count\"), 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used with other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(magrittr)\n",
    "sparkDF %>%\n",
    "    orderBy(desc(sparkDF$count)) %>%\n",
    "    groupBy(\"ORIGIN_COUNTRY_NAME\") %>%\n",
    "    count() %>%limit(10) %>%\n",
    "    collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark's ecosystem and Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many mature packages and projects have been developed as part of the ecosystem of packages. \n",
    "- largest index of Spark Packages at [spark-packages.org](https://spark-packages.org/), where any user can publish to this package repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
