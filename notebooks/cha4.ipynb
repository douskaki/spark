{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4. Structured API Overview\n",
    "\n",
    "There are three core types of distributed collection APIs\n",
    "- Datasets\n",
    "- DataFrames\n",
    "- SQL tables and views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Majority of the Structured APIs apply to both computation\n",
    "- `batch` \n",
    "- `streaming`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "Before proceeding, let’s review the fundamental concepts and definitions that we covered in Part I.\n",
    "Spark is a distributed programming model in which the user specifies transformations. Multiple\n",
    "transformations build up a directed acyclic graph of instructions. An action begins the process of\n",
    "executing that graph of instructions, as a single job, by breaking it down into stages and tasks to\n",
    "execute across the cluster. The logical structures that we manipulate with transformations and actions\n",
    "are DataFrames and Datasets. To create a new DataFrame or Dataset, you call a transformation. To\n",
    "start computation or convert to native language types, you call an action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames and Datasets\n",
    "DataFrames and Datasets are (distributed) table-like collections with well-defined rows and\n",
    "columns. Each column must have the same number of rows as all the other columns (although\n",
    "you can use null to specify the absence of a value) and each column has type information that\n",
    "must be consistent for every row in the collection. To Spark, DataFrames and Datasets represent\n",
    "immutable, lazily evaluated plans that specify what operations to apply to data residing at a\n",
    "location to generate some output. When we perform an action on a DataFrame, we instruct Spark\n",
    "to perform the actual transformations and return the result. These represent plans of how to\n",
    "manipulate rows and columns to compute the user’s desired result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "Tables and views are basically the same thing as DataFrames. We just execute SQL against them\n",
    "instead of DataFrame code. We cover all of this in Chapter 10, which focuses specifically on Spark\n",
    "SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schemas\n",
    "A schema defines the **column names** and **types** of a DataFrame. You can define schemas\n",
    "manually or read a schema from a data source (often called *schema on read*). Schemas consist of\n",
    "types, meaning that you need a way of specifying what lies where."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Structured Spark Types\n",
    "Spark is effectively a programming language of its own. Internally, Spark uses an engine called\n",
    "**Catalyst** that maintains its own type information through the planning and processing of work. In\n",
    "doing so, this opens up a wide variety of execution optimizations that make significant\n",
    "differences. Spark types map directly to the different language APIs that Spark maintains and\n",
    "there exists a lookup table for each of these in Scala, Java, Python, SQL, and R. Even if we use\n",
    "Spark’s Structured APIs from Python or R, the majority of our manipulations will operate strictly\n",
    "on Spark types, not Python types. For example, the following code does not perform addition in\n",
    "Scala or Python; it actually performs addition *purely* in Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "    .master('local[4]')\\\n",
    "    .appName('Cha4')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[(number + 10): bigint]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This addition operation happens because Spark will convert an expression written in an input\n",
    "# language to Spark’s internal Catalyst representation of that same type information.\n",
    "df = spark.range(500).toDF('number')\n",
    "df.select(df[\"number\"] + 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames Versus Datasets\n",
    "- DataFrames : untyped (only check types at *runtime*)\n",
    "- Datasets : typed ( checkt types at *compile time*) \n",
    "    - only available to **Java Virtual Machine (JVM)**– based languages (Scala / Java)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scala**: To Spark , DataFrames = Datasets of Type `Row`. \n",
    "The `“Row”` type is Spark’s internal representation of its optimized\n",
    "in-memory format for computation. This format makes for highly specialized and efficient\n",
    "computation because rather than using JVM types, which can cause high garbage-collection and\n",
    "object instantiation costs, Spark can operate on its own internal format without incurring any of\n",
    "those costs.   \n",
    "**Python or R**: no Dataset: everything is a DataFrame -> always operate on that optimized format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "The internal **Catalyst** format is well covered in numerous Spark presentations.  \n",
    "\n",
    "Talks by [Josh Rosen](https://youtu.be/5ajs8EIPWGI) and [Herman van Hovell](https://youtu.be/GDeePbbCz2g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding **DataFrames**, **Spark Types**, and **Schemas** takes some time to digest.   \n",
    "What you need to know is that when you’re using **DataFrames**, you’re taking advantage of Spark’s **optimized internal format**.   \n",
    "This format applies the same efficiency gains to all of Spark’s language APIs. If you need strict compile-time checking, read Chapter 11 to learn more about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns (cha 5)\n",
    "Represent types:\n",
    "- *simple type* : `integer`, `string`\n",
    "- *complex type* : `array`, `map`, `null`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rows \n",
    "Each record in DataFrame = type `Row`  \n",
    "Created from \n",
    "- SQL\n",
    "- RDDs\n",
    "- data sources\n",
    "- manually from scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0), Row(id=1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(2) .collect() # an array of `Row` objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here just include types for reference. For other languages (Java, Scala), please refer to p58- Spark Types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python\n",
    "from pyspark.sql.types import *\n",
    "b = ByteType()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete reference table. Further documentation [here](http://bit.ly/2EdflXW)\n",
    "\n",
    "![](images/ref1.png)\n",
    "![](images/ref2.png)\n",
    "![](images/ref3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Structured API Execution\n",
    "Execution steps:\n",
    "1. Write DataFrame/Dataset/SQL Code.\n",
    "2. If valid code, Spark converts this to a Logical Plan.\n",
    "3. Spark transforms this Logical Plan to a Physical Plan, checking for optimizations along\n",
    "the way.\n",
    "4. Spark then executes this Physical Plan (RDD manipulations) on the cluster.\n",
    "\n",
    "![](images/exec.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Logical Planning\n",
    "![](images/log.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**logical plan** : convert the user’s set of expressions into the most optimized version. \n",
    "\n",
    "**unresolved logical plan** : unresolved because although your code might be valid, the tables or columns that it refers to might or might\n",
    "not exist. Spark uses the catalog, a repository of all table and DataFrame information, to resolve\n",
    "columns and tables in the *analyzer*. The analyzer might reject the unresolved logical plan if the\n",
    "required table or column name does not exist in the catalog. \n",
    "\n",
    "**Logical Optimization** : If the analyzer can resolve it, the\n",
    "result is passed through the Catalyst Optimizer, a collection of rules that attempt to optimize the\n",
    "logical plan by pushing down predicates or selections. Packages can extend the Catalyst to\n",
    "include their own rules for domain-specific optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physical planning \n",
    "After successfully creating an optimized logical plan, Spark then begins the physical planning\n",
    "process. The physical plan, often called a **Spark plan**, specifies how the logical plan will execute\n",
    "on the cluster by generating different physical execution strategies and comparing them through\n",
    "a cost model, as depicted in Figure 4-3. An example of the cost comparison might be choosing\n",
    "how to perform a given join by looking at the physical attributes of a given table (how big the\n",
    "table is or how big its partitions are).\n",
    "\n",
    "![](images/phy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Physical planning** ----->>>> a series of *RDDs* and *transformations.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution\n",
    "Upon selecting a physical plan, Spark runs all of this code over RDDs, the lower-level\n",
    "programming interface of Spark ( Part III). Spark performs further\n",
    "*optimizations at runtime*, generating native Java bytecode that can remove entire tasks or stages\n",
    "during execution. Finally the result is returned to the user."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
