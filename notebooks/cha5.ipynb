{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5. Basic Structured Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter focuses exclusively on fundamental\n",
    "DataFrame operations and avoids aggregations, window functions, and joins. These are\n",
    "discussed in subsequent chapters.\n",
    "\n",
    "**DataFrame**:\n",
    "- `records` = type `Row`\n",
    "- `columns` : represent a computation expression that can be performed on each individual record in the Dataset\n",
    "- `Schemas` : *name* and *type* of data in each column\n",
    "- `Partitioning` : layout of the DataFrame (DataSet)'s physical distribution across the cluster\n",
    "- `partitioning scheme` : defines how that (partitioning) is allocated. (either based on values in a certain column or nondeterministically.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "    .master('local[4]')\\\n",
    "    .appName('Cha5')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"json\").load(\"../Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schemas\n",
    "Defines: \n",
    "- column names\n",
    "- column types\n",
    "\n",
    "Two ways to define schemas:\n",
    "- let a data source to define the schema (*schema-on-read*)\n",
    "- define it explicitely \n",
    "\n",
    "**Warning**:  \n",
    "Deciding whether you need to define a schema prior to reading in your data depends on your use case.\n",
    "For ad hoc analysis, schema-on-read usually works just fine (although at times it can be a bit slow with\n",
    "plain-text file formats like CSV or JSON). However, this can also lead to precision issues like a long\n",
    "type incorrectly set as an integer when reading in a file. When using Spark for production `Extract,Transform, and Load (ETL)`, it is often a good idea to define your schemas manually, especially when\n",
    "working with untyped data sources like CSV and JSON because schema inference can vary depending\n",
    "on the type of data that you read in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format('json').load('../Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json').schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Schema**: \n",
    "- *StructType*  \n",
    "    - `StructField`( *name*, *type*, *Boolean flag*, *metadata (optional)*)\n",
    "    - `StructField`( *name*, *type*, *Boolean flag*, *metadata (optional)*)\n",
    "    - ...\n",
    "    \n",
    "*** Boolean flag: specifies whether that column can contain missing or `null` values\n",
    "\n",
    "If the types in the data (at runtime) do not match\n",
    "the schema, Spark will throw an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a specified schema\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "myManualSchema = StructType([\n",
    "    StructField('DEST_COUNTRY_NAME', StringType(), True),\n",
    "    StructField('ORIGIN_COUNTRY_NAME', StringType(), True),\n",
    "    StructField('count', LongType(), False, metadata = {'hello' : 'world'})\n",
    "])\n",
    "\n",
    "df = spark.read.format('json').schema(myManualSchema)\\\n",
    "    .load('../Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns and Expressions\n",
    "\n",
    "### **Columns**\n",
    "Construct and refert to a column : \n",
    "- `column`\n",
    "- `col` (they are equal to each other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'someColumnName'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, column\n",
    "col('someColumnName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'someColumnName'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column('someColumnName')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : column might or might not exist in our *DataFrames*. Columns are not resolved until we compare the column names with\n",
    "those we are maintaining in the `catalog`. Column and table resolution happens in the `analyzer` phase, as discussed in Chapter 4.\n",
    "\n",
    "**Side note for Scala**:\n",
    "\n",
    "We just mentioned two different ways of referring to columns. Scala has some unique language\n",
    "features that allow for more shorthand ways of referring to columns. The following bits of syntactic\n",
    "sugar perform the exact same thing, namely creating a column, but provide no performance\n",
    "improvement:\n",
    "\n",
    "```scala\n",
    "// in Scala\n",
    "$\"myColumn\"\n",
    "'myColumn\n",
    "```\n",
    "\n",
    "The $ allows us to designate a string as a special string that should refer to an expression. The tick\n",
    "mark (') is a special thing called a symbol; this is a Scala-specific construct of referring to some\n",
    "identifier. They both perform the same thing and are shorthand ways of referring to columns by name.\n",
    "You’ll likely see all of the aforementioned references when you read different people’s Spark code.\n",
    "We leave it to you to use whatever is most comfortable and maintainable for you and those with whom\n",
    "you work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit column references\n",
    "Refer to a specific DataFrame’s column: use `col` method on a DataFrame  \n",
    "**an added benefit** : Spark does not need to resolve this column itself (during the analyzer phase) because we did that for Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'count'>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'DEST_COUNTRY_NAME'>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.DEST_COUNTRY_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expressions\n",
    "Columns are expressions\n",
    "**expression:** a set of transformations on one or more values in a record in a DataFrame.\n",
    "\n",
    "In the simplest case, an expression, created via the `expr` function, is just a DataFrame column\n",
    "reference. In the simplest case, `expr(\"someCol\")` is equivalent to `col(\"someCol\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'count'>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "expr('count') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'count'>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Columns as expressions**\n",
    "Columns provide a subset of expression functionality. If you use `col()` and want to perform\n",
    "transformations on that column, you must perform those on that column reference. When using\n",
    "an expression, the `expr` function can actually parse transformations and column references from\n",
    "a string and can subsequently be passed into further transformations. \n",
    "\n",
    "Examples:\n",
    "\n",
    "- `expr('someCol - 5')` \n",
    "- `col('someCol') - 5`\n",
    "- `expr(\"someCol\") - 5`\n",
    "\n",
    "They are equivalent because Spark compiles these to a logical tree specifying the order of operations.\n",
    "\n",
    "Key points:\n",
    "- Columns are just expressions.\n",
    "- Columns and transformations of those columns compile to the same logical plan as\n",
    "parsed expressions.\n",
    "\n",
    "Example \n",
    "```\n",
    "(((col(\"someCol\") + 5) * 200) - 6) < col(\"otherCol\")\n",
    "```\n",
    "\n",
    "![](images/log-tree.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'((((someCol + 5) * 200) - 6) < OtherCol)'>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expr('(((someCol + 5) * 200)  - 6) < OtherCol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "SQL expression and DataFrame code compile to the same underlying logical tree prior to execution.\n",
    "\n",
    "This means that you can write your expressions as DataFrame code or as SQL expressions and get the exact **same performance characteristics**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing a DataFrame’s columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## .columns would print out the columns of a dataframe as a list\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Records and Rows\n",
    "single record = `Row`\n",
    "\n",
    "`Row` : internally, arrays of bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Rows\n",
    "if you create a `Row` manually, you must specify the values in the same order as the schema of the DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "myRow = Row('Hello', None, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello \n",
      " 1\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    myRow[0],\n",
    "    \"\\n\",\n",
    "    myRow[2]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Transformations\n",
    "core operations:\n",
    "- add rows or columns\n",
    "- remove rows or columns\n",
    "- transform a row into a column (or vice versa)\n",
    "- change the order of rows based on the values in columns\n",
    "![](images/trans.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames (more details in cha9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create DataFrame from data source\n",
    "df = spark.read.format('json').load('../Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json')\n",
    "df.createOrReplaceTempView('dfTable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| some| col|names|\n",
      "+-----+----+-----+\n",
      "|Hello|null|    1|\n",
      "|World|null|    2|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## create on the fly\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "# specify schemas\n",
    "myManualSchema = StructType([\n",
    "    StructField(\"some\", StringType(), True),\n",
    "    StructField(\"col\", StringType(), True),\n",
    "    StructField(\"names\", LongType(), False)\n",
    "])\n",
    "\n",
    "# create rows\n",
    "myRow1 = Row(\"Hello\", None, 1)\n",
    "myRow2 = Row(\"World\", None, 2)\n",
    "\n",
    "# create DataFrame with Rows and Schemas\n",
    "myDf = spark.createDataFrame([myRow1, myRow2], myManualSchema)\n",
    "\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Three most useful transformations**:\n",
    "- `select` : with columns and expressions\n",
    "- `selectExpr` : with expressions in strings\n",
    "- `pyspark.sql.functions` : a group of functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **select and selectExpr**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`select` and `selectExpr` allow you to do the DataFrame **equivalent of SQL queries** on a table of data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "-- 1.\n",
    "-- in SQL\n",
    "SELECT DEST_COUNTRY_NAME \n",
    "FROM dfTable \n",
    "LIMIT 2\n",
    "```\n",
    "\n",
    "equivalent to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('DEST_COUNTRY_NAME').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "-- 2.\n",
    "-- in SQL\n",
    "SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME \n",
    "FROM dfTable \n",
    "LIMIT 2\n",
    "```\n",
    "equivalent to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "+-----------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-----------------+-----------------+-----------------+-----------------+\n",
      "|    United States|    United States|    United States|    United States|\n",
      "|    United States|    United States|    United States|    United States|\n",
      "+-----------------+-----------------+-----------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## refer to columns in a number of different ways\n",
    "df.select(\n",
    "    'DEST_COUNTRY_NAME',\n",
    "    expr('DEST_COUNTRY_NAME'),\n",
    "    col(\"DEST_COUNTRY_NAME\"),\n",
    "    column('DEST_COUNTRY_NAME')\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`expr` is the most flexible. It can refer to:\n",
    "- plain column\n",
    "- a string manipulation of a column\n",
    "\n",
    "```sql\n",
    "-- in SQL\n",
    "SELECT DEST_COUNTRY_NAME as destination \n",
    "FROM dfTable \n",
    "LIMIT 2\n",
    "```\n",
    "equivalent to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|  destination|\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr('DEST_COUNTRY_NAME as destination')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|change_to_something_else|\n",
      "+------------------------+\n",
      "|           United States|\n",
      "|           United States|\n",
      "+------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    expr('DEST_COUNTRY_NAME as destination')\\\n",
    "    .alias('change_to_something_else')\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`selectExpr` : combine `select` wih `expr`, most convenient interface for everyday use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|  destination|DEST_COUNTRY_NAME|\n",
      "+-------------+-----------------+\n",
      "|United States|    United States|\n",
      "|United States|    United States|\n",
      "+-------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr('DEST_COUNTRY_NAME as destination', 'DEST_COUNTRY_NAME').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`selectExpr` : a simple way to build up complex expressions\n",
    "\n",
    "```sql\n",
    "-- in SQL\n",
    "SELECT *, (DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\n",
    "FROM dfTable\n",
    "LIMIT 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adds a new column withinCountry\n",
    "df.selectExpr(\n",
    "    '*',\n",
    "    '(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry'    \n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aggregation**\n",
    "```sql\n",
    "-- in SQL\n",
    "SELECT avg(count), count(distinct(DEST_COUNTRY_NAME)) \n",
    "FROM dfTable \n",
    "LIMIT 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------+\n",
      "| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n",
      "+-----------+---------------------------------+\n",
      "|1770.765625|                              132|\n",
      "+-----------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\n",
    "    'avg(count)',\n",
    "    'count(distinct(DEST_COUNTRY_NAME))'\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Converting to Spark Types (Literals)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`literals`** : explicit values that are just a value (**rather than a new column**). This might be a constant value or something we’ll need to compare to later on.   \n",
    "similar to **`mutate()`** in R\n",
    "\n",
    "using `lit()`\n",
    "\n",
    "```sql\n",
    "-- in SQL\n",
    "SELECT *, 1 as One \n",
    "FROM dfTable \n",
    "LIMIT 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.select(\n",
    "    expr('*'),\n",
    "    lit(1).alias('One')\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\n",
    "    '*',\n",
    "    '1 as One'\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Adding Columns**\n",
    "more formal way : using `.withColumn` method.\n",
    "\n",
    "`.withColumn`: take two arguments\n",
    "- column name\n",
    "- expression\n",
    "\n",
    "```sql\n",
    "-- in SQL\n",
    "SELECT *, 1 as numberOne \n",
    "FROM dfTable \n",
    "LIMIT 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\n",
    "    'numberOne', lit(1)\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## another more complicated one\n",
    "df.withColumn(\n",
    "    'withinCountry',\n",
    "    expr('ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME')\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count', 'Destination']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename column\n",
    "df.withColumn(\n",
    "    'Destination',\n",
    "    expr('DEST_COUNTRY_NAME')\n",
    ").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Renaming Columns**\n",
    "`.withColumnRenamed`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dest', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumnRenamed(\n",
    "    'DEST_COUNTRY_NAME',\n",
    "    'dest'\n",
    ").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Reserved Characters and Keywords**\n",
    "using backtick ( **\\`** ) characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|This Long Column-Name|\n",
      "+-----------------+-------------------+-----+---------------------+\n",
      "|    United States|            Romania|   15|              Romania|\n",
      "|    United States|            Croatia|    1|              Croatia|\n",
      "+-----------------+-------------------+-----+---------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithLongColName = df.withColumn(\n",
    "    'This Long Column-Name',\n",
    "    expr('ORIGIN_COUNTRY_NAME')\n",
    ")\n",
    "dfWithLongColName.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don’t need escape characters here because the first argument to withColumn is just a string\n",
    "for the new column name. In this example, however, we need to use backticks because we’re\n",
    "referencing a column in an expression:\n",
    "```sql\n",
    "-- in SQL\n",
    "SELECT `This Long Column-Name`, `This Long Column-Name` as `new col`\n",
    "FROM dfTableLong \n",
    "LIMIT 2\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------+\n",
      "|This Long Column-Name|new col|\n",
      "+---------------------+-------+\n",
      "|              Romania|Romania|\n",
      "|              Croatia|Croatia|\n",
      "+---------------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithLongColName.selectExpr(\n",
    "    '`This Long Column-Name`',\n",
    "    '`This Long Column-Name` as `new col`'\n",
    ").show(2)\n",
    "\n",
    "# create sql temp view\n",
    "dfWithLongColName.createOrReplaceTempView('dfTableLong')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **col()** : *explicit string-to-column reference, which is interpreted as a `literal`* - **not escape**\n",
    "- **expr()** : *expression* - **escape**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This Long Column-Name']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using `col()`\n",
    "dfWithLongColName.select(col(\"This Long Column-Name\")).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This Long Column-Name']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using `expr()`\n",
    "dfWithLongColName.select(expr(\"`This Long Column-Name`\")).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Case Sensitivity**\n",
    "\n",
    "By default Spark is case insensitive; however, you can make Spark case sensitive by setting the\n",
    "configuration:\n",
    "\n",
    "```sql\n",
    "-- in SQL\n",
    "set spark.sql.caseSensitive true\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Removing Columns**\n",
    "`.drop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop('ORIGIN_COUNTRY_NAME').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['count', 'This Long Column-Name']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfWithLongColName.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Changing a Column’s Type (cast)**\n",
    "convert our count column from an `integer` to a type `Long`\n",
    "```sql\n",
    "-- in SQL\n",
    "SELECT *, cast(count as long) AS count2 \n",
    "FROM dfTable\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint, count2: bigint]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\n",
    "    'count2',\n",
    "    col('count').cast('long')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint, count2: string]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## convert ot string\n",
    "df.withColumn(\n",
    "    'count2',\n",
    "    col('count').cast('string')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Filtering Rows**\n",
    "Two methods (they are the same)\n",
    "- `.filter`\n",
    "- `.where`\n",
    "\n",
    "**Note**  \n",
    "When using the Dataset API from either *Scala or Java*, filter also accepts an **arbitrary function** that\n",
    "Spark will apply to each record in the Dataset. See Chapter 11 for more information.\n",
    "\n",
    "```sql\n",
    "-- in SQL\n",
    "SELECT * \n",
    "FROM dfTable \n",
    "WHERE count < 2 \n",
    "LIMIT 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where('count < 2').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(expr('count < 2')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col('count') < 2).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**multiple filters** : multiple AND filters - chain them sequentially. \n",
    "\n",
    "*because Spark automatically performs all filtering operations at\n",
    "the same time regardless of the filter ordering.*\n",
    "\n",
    "```sql\n",
    "-- in SQL\n",
    "SELECT * \n",
    "FROM dfTable \n",
    "WHERE count < 2 AND ORIGIN_COUNTRY_NAME != \"Croatia\"\n",
    "LIMIT 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(expr('count < 2'))\\\n",
    "    .where(col('ORIGIN_COUNTRY_NAME') != \"Croatia\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Getting Unique Rows**\n",
    "- `.distinct()`\n",
    "\n",
    "```sql\n",
    "-- in SQL\n",
    "SELECT COUNT(DISTINCT(ORIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME)) \n",
    "FROM dfTable\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\n",
    "    'ORIGIN_COUNTRY_NAME',\n",
    "    'DEST_COUNTRY_NAME'\n",
    ").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "-- in SQL\n",
    "SELECT COUNT(DISTINCT ORIGIN_COUNTRY_NAME) \n",
    "FROM dfTable\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\n",
    "    'ORIGIN_COUNTRY_NAME'\n",
    ").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Random Samples**\n",
    "- `.sample()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 5\n",
    "withReplcement = False\n",
    "fraction= 0.5\n",
    "df.sample(withReplcement, fraction, seed).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Random Splits**\n",
    "- `.randomSplit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrames = df.randomSplit([0.25, 0.75], seed = 5)\n",
    "dataFrames[0].count() > dataFrames[1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint],\n",
       " DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(dataFrames))\n",
    "dataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count() == dataFrames[0].count() + dataFrames[1].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Concatenating and Appending Rows (Union)**\n",
    "DataFrames are `immutable` --- >>>> cannot append to DataFrames \n",
    "\n",
    "Solution: `union` the original DataFrame with the new DataFrame.\n",
    "\n",
    "Makre sure:\n",
    "- same schemas\n",
    "- same number of columns\n",
    "\n",
    "**WARNING**  \n",
    "`Unions` are currently performed based on location, not on the schema. This means that columns will not automatically line up the way you think they might."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# copy same schema\n",
    "schema = df.schema\n",
    "\n",
    "# create new rows\n",
    "newRows = [\n",
    "    Row(\"New Country\", \"Other Country\", 5),\n",
    "    Row(\"New Country 2\", \"Other Country 3\", 1)\n",
    "]\n",
    "\n",
    "# parallelize the rows\n",
    "parallelizedRows = spark.sparkContext.parallelize(newRows)\n",
    "\n",
    "# create new DF\n",
    "newDF = spark.createDataFrame(parallelizedRows, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|          Gibraltar|    1|\n",
      "|    United States|             Cyprus|    1|\n",
      "|    United States|            Estonia|    1|\n",
      "|    United States|          Lithuania|    1|\n",
      "|    United States|           Bulgaria|    1|\n",
      "|    United States|            Georgia|    1|\n",
      "|    United States|            Bahrain|    1|\n",
      "|    United States|   Papua New Guinea|    1|\n",
      "|    United States|         Montenegro|    1|\n",
      "|    United States|            Namibia|    1|\n",
      "|    New Country 2|    Other Country 3|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.union(newDF)\\\n",
    "    .where('count = 1')\\\n",
    "    .where(col('ORIGIN_COUNTRY_NAME') != 'United States')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~ You’ll need to use this new DataFrame reference in order to refer to the DataFrame\n",
    "with the newly appended rows.   \n",
    "\n",
    "A common way to do this \n",
    "- make the DataFrame into a view\n",
    "- register it as a table so that you can reference it more dynamically in your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Sorting Rows**\n",
    "two equivalent operations:   \n",
    "*default = ascending*\n",
    "- `sort`\n",
    "- `orderBy`\n",
    "\n",
    "accept both \n",
    "- column expressions\n",
    "- strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort('count').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy('count', 'DEST_COUNTRY_NAME').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(\n",
    "    col('count'), \n",
    "    col('DEST_COUNTRY_NAME')\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify sorting directions:\n",
    "- `desc`\n",
    "- `asc`\n",
    "\n",
    "```sql\n",
    "-- in SQL\n",
    "SELECT * \n",
    "FROM dfTable \n",
    "ORDER BY count DESC, DEST_COUNTRY_NAME ASC \n",
    "LIMIT 2\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Moldova|      United States|    1|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc, asc\n",
    "df.orderBy(\n",
    "    expr('count desc')\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(\n",
    "    col('count').desc(), col('DEST_COUNTRY_NAME').asc()\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(\n",
    "    desc('count'), asc('DEST_COUNTRY_NAME')\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced tip : specify where you would like your null values to appear in an ordered DataFrame. \n",
    "\n",
    "( Not currently supported in `pyspark -v 2.3.0` ; new pull request have been made, check newest version in GitHub.) \n",
    "- `asc_nulls_first`, `desc_nulls_first`\n",
    "- `asc_nulls_last`, `desc_nulls_last`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For optimization, it is advisable to sort within partition before anothe transformation. (more in part III)\n",
    "- `.sortWithinPartitions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format('json').load('../Spark-The-Definitive-Guide-master/data/flight-data/json/*-summary.json')\\\n",
    "    .sortWithinPartitions('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Limit**\n",
    "- `.limit`\n",
    "\n",
    "restrict what you extract from a DataFrame. \n",
    "\n",
    "```sql\n",
    "-- in SQL\n",
    "SELECT * \n",
    "FROM dfTable \n",
    "LIMIT 6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(6).show()\n",
    "# different from `df.show(6)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "-- in SQL\n",
    "SELECT * \n",
    "FROM dfTable \n",
    "ORDER BY count desc \n",
    "LIMIT 6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "|             Moldova|      United States|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(expr('count desc')).limit(6).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Repartition and Coalesce**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important optimization opportunity is to partition the data according to some frequently\n",
    "filtered columns, which control the physical layout of data across the cluster including the\n",
    "*partitioning scheme* and the *number of partitions*.\n",
    "\n",
    "**Repartition** will incur a full shuffle of the data, regardless of whether one is necessary. This\n",
    "means that you should typically only repartition when the future number of partitions is greater\n",
    "than your current number of partitions or when you are looking to partition by a set of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current number of partitions : 1\n"
     ]
    }
   ],
   "source": [
    "print('Current number of partitions : %d' % df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartitioned number : 5\n"
     ]
    }
   ],
   "source": [
    "p = df.repartition(5).rdd.getNumPartitions()\n",
    "print('Repartitioned number : %d' % p )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you will be filtering a by certain **column** frequently, may repartition based on that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition('DEST_COUNTRY_NAME')\n",
    "df.repartition(col('DEST_COUNTRY_NAME'))\n",
    "\n",
    "df.repartition(5, col('DEST_COUNTRY_NAME'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coalesce** will not incur a full shuffle and will try to combine partitions. This\n",
    "operation will shuffle your data into five partitions based on the destination country name, and\n",
    "then coalesce them (without a full shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(5, col('DEST_COUNTRY_NAME')).coalesce(2)\\\n",
    "    .rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Collecting Rows to the Driver**\n",
    "Spark maintains the state of the cluster in the driver. \n",
    "\n",
    "To collect some of your data to the driver in order to manipulate it on your local machine.\n",
    "- `collect` : gets all data from the entire DataFrame\n",
    "- `take` : select the first N rows\n",
    "- `show` : print out a number of N rows *nicely*\n",
    "- `toLocalIterator` : collects partitions to the driver as an iterator. ("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**  \n",
    "Any collection of data to the driver can be a very expensive operation! If you have a large dataset and\n",
    "call `collect`, you can crash the driver. If you use `toLocalIterator` and have very large partitions,\n",
    "you can easily crash the driver node and lose the state of your application. This is also expensive\n",
    "because we can operate on a one-by-one basis, instead of running computation in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectDF = df.limit(10)\n",
    "collectDF.take(5) # take works with an Integer count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Grenada', count=62),\n",
       " Row(DEST_COUNTRY_NAME='Costa Rica', ORIGIN_COUNTRY_NAME='United States', count=588),\n",
       " Row(DEST_COUNTRY_NAME='Senegal', ORIGIN_COUNTRY_NAME='United States', count=40),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectDF.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "collectDF.show() # this prints out nicely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "|Egypt            |United States      |15   |\n",
      "|United States    |India              |62   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "collectDF.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To iterate over the entire dataset \n",
    "- `toLocalIterator` : collects partitions to the driver as an iterator. ( This\n",
    "method allows you to iterate over the entire dataset partition-by-partition in a serial manner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<itertools.chain at 0x7fc2da001fd0>"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectDF.toLocalIterator()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
